@inproceedings{10.1145/3461648.3463842,
author = {Poroor, Jayaraj and Lal, Akash and Ghanta, Sandesh},
title = {Robust I/O-Compute Concurrency for Machine Learning Pipelines in Constrained Cyber-Physical Devices},
year = {2021},
isbn = {9781450384728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461648.3463842},
doi = {10.1145/3461648.3463842},
abstract = {Cyberphysical systems have numerous industrial and commercial applications. Such systems are often built using low-resource devices that gather and process data, using machine-learning (ML) models, to make intelligent decisions and provide value to users. Programming such low-resource devices with an impoverished system runtime is often challenging. This paper presents a new domain-specific language called PiCon for programming ML pipelines in low-resource devices. PiCon allows safe I/O-compute concurrency, ruling out a large class of errors, while providing a simple and sequential coding abstraction to the programmer. PiCon compiles to C code and easily interfaces with existing C/C++ code. Furthermore, the generated code does not rely on multi-threading support or dynamic memory allocation, dramatically reducing its footprint on the device. We present experience porting two real-world ML applications that demonstrate simplification in programmability, in addition to several safe-by-construction guarantees.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {1–11},
numpages = {11},
keywords = {Cyberphysical systems, Constrained devices, Embedded systems, IoT, Machine Learning},
location = {Virtual, Canada},
series = {LCTES 2021}
}

@article{10.14778/3007263.3007279,
author = {Boehm, Matthias and Dusenberry, Michael W. and Eriksson, Deron and Evfimievski, Alexandre V. and Manshadi, Faraz Makari and Pansare, Niketan and Reinwald, Berthold and Reiss, Frederick R. and Sen, Prithviraj and Surve, Arvind C. and Tatikonda, Shirish},
title = {SystemML: Declarative Machine Learning on Spark},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007279},
doi = {10.14778/3007263.3007279},
abstract = {The rising need for custom machine learning (ML) algorithms and the growing data sizes that require the exploitation of distributed, data-parallel frameworks such as MapReduce or Spark, pose significant productivity challenges to data scientists. Apache SystemML addresses these challenges through declarative ML by (1) increasing the productivity of data scientists as they are able to express custom algorithms in a familiar domain-specific language covering linear algebra primitives and statistical functions, and (2) transparently running these ML algorithms on distributed, data-parallel frameworks by applying cost-based compilation techniques to generate efficient, low-level execution plans with in-memory single-node and large-scale distributed operations. This paper describes SystemML on Apache Spark, end to end, including insights into various optimizer and runtime techniques as well as performance characteristics. We also share lessons learned from porting SystemML to Spark and declarative ML in general. Finally, SystemML is open-source, which allows the database community to leverage it as a testbed for further research.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {1425–1436},
numpages = {12}
}

@inproceedings{10.1145/3468044.3468052,
author = {Podobas, Artur and Svedin, Martin and Chien, Steven W. D. and Peng, Ivy B. and Ravichandran, Naresh Balaji and Herman, Pawel and Lansner, Anders and Markidis, Stefano},
title = {StreamBrain: An HPC Framework for Brain-like Neural Networks on CPUs, GPUs and FPGAs},
year = {2021},
isbn = {9781450385497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468044.3468052},
doi = {10.1145/3468044.3468052},
abstract = {The modern deep learning method based on backpropagation has surged in popularity and has been used in multiple domains and application areas. At the same time, there are other - less-known - machine learning algorithms with a mature and solid theoretical foundation whose performance remains unexplored. One such example is the brain-like Bayesian Confidence Propagation Neural Network (BCPNN). In this paper, we introduce StreamBrain--a framework that allows neural networks based on BCPNN to be practically deployed in High-Performance Computing systems. StreamBrain is a domain-specific language (DSL), similar in concept to existing machine learning (ML) frameworks, and supports backends for CPUs, GPUs, and even FPGAs. We empirically demonstrate that StreamBrain can train the well-known ML benchmark dataset MNIST within seconds, and we are the first to demonstrate BCPNN on STL-10 size networks. We also show how StreamBrain can be used to train with custom floating-point formats and illustrate the impact of using different bfloat variations on BCPNN using FPGAs.},
booktitle = {Proceedings of the 11th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
articleno = {8},
numpages = {6},
keywords = {Unsupervised learning, HPC, GPU, Neural networks, Emerging Machine Learning, AI, FPGA, BCPNN, Representation learning},
location = {Online, Germany},
series = {HEART '21}
}

@inproceedings{10.1145/3510458.3513012,
author = {Alves, Lucas and Pereira, Jos\'{e} Davi and Arag\~{a}o, Nat\'{a}lia and Chagas, Matheus and Maia, Paulo Henrique},
title = {DRESS-ML: A Domain-Specific Language for Modelling Exceptional Scenarios and Self-Adaptive Behaviours for Drone-Based Applications},
year = {2022},
isbn = {9781450392273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510458.3513012},
doi = {10.1145/3510458.3513012},
abstract = {Drones are gaining attention due to its possibility to support wide different types of applications. Since they can operate in different environments, it is possible to encounter uncertainties and exceptional situations, not initially predicted, during the use of drone-based applications. In this realm, self-adaptive strategies have been successfully used to guarantee resilience and continuous execution of such applications despite environment changes. Although some modelling approaches emerged to represent drone concepts, they are limited to model only expected flight plans or include few environmental conditions and drone resources, which restrict considerably their use. To mitigate those problems, this work proposes a domain-specific language, called DRESS-ML, which allows modelling exceptional situations and self-adaptive behaviours for drone-based applications. It relies on the Given-When-Then template used in the Behaviour-driven development (BDD) technique and the some of the main Aspect-oriented Programming concepts. We validate the applicability of our language through a proof of concept regarding an example application that uses a drone to monitor a forest to search for fire spots.Drones are gaining attention due to their possibility of supporting diverse applications and environments, such as search-and-rescue, surveillance, and goods delivery. Due to this variety of applications, drones can face both uncertainties and exceptional situations that they did not initially foresee during flight plan. In this sense, providing the ability to monitor the system and its environment and to change the system to ensure resilience and continuous execution are benefits of self-adaptation strategies. Although some studies have proposed approaches to model drone concepts, they are limited to model only expected flight plans or include few environmental conditions and drone resources, which restrict considerably their use. This work proposes a domain-specific language, called DRESS-ML, which allows modelling exceptional scenarios and self-adaptive behaviors to mitigate those problems. The language is based on an well known structure used for specifying system behaviour and the main concepts of a programming paradigm that injects encapsulated behaviours in the system. A practical example that uses a drone to monitor a forest to search for fire spots is used to evaluate the proposed language, demonstrating its applicability to model various exceptional scenarios.},
booktitle = {Proceedings of the 2022 ACM/IEEE 44th International Conference on Software Engineering: Software Engineering in Society},
pages = {56–66},
numpages = {11},
keywords = {modelling language, exceptional scenarios, self-adaptive systems, drones},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEIS '22}
}

@article{10.14778/3342263.3342633,
author = {Kunft, Andreas and Katsifodimos, Asterios and Schelter, Sebastian and Bre\ss{}, Sebastian and Rabl, Tilmann and Markl, Volker},
title = {An Intermediate Representation for Optimizing Machine Learning Pipelines},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342633},
doi = {10.14778/3342263.3342633},
abstract = {Machine learning (ML) pipelines for model training and validation typically include preprocessing, such as data cleaning and feature engineering, prior to training an ML model. Preprocessing combines relational algebra and user-defined functions (UDFs), while model training uses iterations and linear algebra. Current systems are tailored to either of the two. As a consequence, preprocessing and ML steps are optimized in isolation. To enable holistic optimization of ML training pipelines, we present Lara, a declarative domain-specific language for collections and matrices. Lara's inter-mediate representation (IR) reflects on the complete program, i.e., UDFs, control flow, and both data types. Two views on the IR enable diverse optimizations. Monads enable operator pushdown and fusion across type and loop boundaries. Combinators provide the semantics of domain-specific operators and optimize data access and cross-validation of ML algorithms. Our experiments on preprocessing pipelines and selected ML algorithms show the effects of our proposed optimizations on dense and sparse data, which achieve speedups of up to an order of magnitude.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1553–1567},
numpages = {15}
}

@inproceedings{10.1145/3314221.3314597,
author = {Gopinath, Sridhar and Ghanathe, Nikhil and Seshadri, Vivek and Sharma, Rahul},
title = {Compiling KB-Sized Machine Learning Models to Tiny IoT Devices},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314597},
doi = {10.1145/3314221.3314597},
abstract = {Recent advances in machine learning (ML) have produced KiloByte-size models that can directly run on constrained IoT devices. This approach avoids expensive communication between IoT devices and the cloud, thereby enabling energy-efficient real-time analytics. However, ML models are expressed typically in floating-point, and IoT hardware typically does not support floating-point. Therefore, running these models on IoT devices requires simulating IEEE-754 floating-point using software, which is very inefficient. We present SeeDot, a domain-specific language to express ML inference algorithms and a compiler that compiles SeeDot programs to fixed-point code that can efficiently run on constrained IoT devices. We propose 1)&nbsp;a novel compilation strategy that reduces the search space for some key parameters used in the fixed-point code, and 2)&nbsp;new efficient implementations of expensive operations. SeeDot compiles state-of-the-art KB-sized models to various microcontrollers and low-end FPGAs. We show that SeeDot outperforms 1) software emulation of floating-point (Arduino), 2) high-bitwidth fixed-point (MATLAB), 3) post-training quantization (TensorFlow-Lite), and 4) floating- and fixed-point FPGA implementations generated using high-level synthesis tools.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {79–95},
numpages = {17},
keywords = {Programming Language, Fixed-point, Compiler, FPGA, Microcontroller, Machine Learning, IoT device},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/1134285.1134311,
author = {Verbaere, Mathieu and Ettinger, Ran and de Moor, Oege},
title = {JunGL: A Scripting Language for Refactoring},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134311},
doi = {10.1145/1134285.1134311},
abstract = {Refactorings are behaviour-preserving program transformations, typically for improving the structure of existing code. A few of these transformations have been mechanised in interactive development environments. Many more refactorings have been proposed, and it would be desirable for programmers to script their own refactorings. Implementing such source-to-source transformations, however, is quite complex: even the most sophisticated development environments contain significant bugs in their refactoring tools.We present a domain-specific language for refactoring, named JunGL. It manipulates a graph representation of the program: all information about the program, including ASTs for its compilation units, variable binding, control flow and so on is represented in a uniform graph format. The language is a hybrid of a functional language (in the style of ML) and a logic query language (akin to Datalog). JunGL furthermore has a notion of demand-driven evaluation for constructing computed information in the graph, such as control flow edges. Borrowing from earlier work on the specification of compiler optimisations, JunGL uses so-called `path queries' to express dataflow properties.We motivate the design of JunGL via a number of non-trivial refactorings, and describe its implementation on the.NET platform.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {172–181},
numpages = {10},
keywords = {refactoring, source code transformation, language workbenches, scripting language},
location = {Shanghai, China},
series = {ICSE '06}
}

@article{10.14778/3236187.3236188,
author = {Mahajan, Divya and Kim, Joon Kyung and Sacks, Jacob and Ardalan, Adel and Kumar, Arun and Esmaeilzadeh, Hadi},
title = {In-RDBMS Hardware Acceleration of Advanced Analytics},
year = {2018},
issue_date = {July 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3236187.3236188},
doi = {10.14778/3236187.3236188},
abstract = {The data revolution is fueled by advances in machine learning, databases, and hardware design. Programmable accelerators are making their way into each of these areas independently. As such, there is a void of solutions that enables hardware acceleration at the intersection of these disjoint fields. This paper sets out to be the initial step towards a unifying solution for in-Database Acceleration of Advanced Analytics (DAnA). Deploying specialized hardware, such as FPGAs, for in-database analytics currently requires hand-designing the hardware and manually routing the data. Instead, DAnA automatically maps a high-level specification of advanced analytics queries to an FPGA accelerator. The accelerator implementation is generated for a User Defined Function (UDF), expressed as a part of an SQL query using a Python-embedded Domain-Specific Language (DSL). To realize an efficient in-database integration, DAnA accelerators contain a novel hardware structure, Striders, that directly interface with the buffer pool of the database. Striders extract, cleanse, and process the training data tuples that are consumed by a multi-threaded FPGA engine that executes the analytics algorithm. We integrate DAnA with PostgreSQL to generate hardware accelerators for a range of real-world and synthetic datasets running diverse ML algorithms. Results show that DAnA-enhanced PostgreSQL provides, on average, 8.3\texttimes{} end-to-end speedup for real datasets, with a maximum of 28.2\texttimes{}. Moreover, DAnA-enhanced PostgreSQL is, on average, 4.0\texttimes{} faster than the multi-threaded Apache MADLib running on Greenplum. DAnA provides these benefits while hiding the complexity of hardware design from data scientists and allowing them to express the algorithm in ≈30-60 lines of Python.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1317–1331},
numpages = {15}
}

@inbook{10.1145/3447404.3447405,
title = {Preface},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447405},
abstract = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice provides a comprehensive resource on what has become the dominant paradigm for novel interaction design methods involving gesture, speech, text, and touch embedded in novel and emerging interfaces. These interfaces support smartphones, wearables, in-vehicle devices, virtual reality, robotic, the Internet of Things (IoT), brain–computer interaction, and many other applications that are now highly competitive commercially.This edited collection is written by international experts and pioneers in the field of digital signal processing (DSP) and machine learning (ML) for interactive systems. It provides a textbook for students, and a reference and technology roadmap for developers and professionals working in interaction design on emerging platforms. This introductory textbook presents theory chapters on statistical grounding, signal processing, and ML foundations for guiding the development of novel interactive systems. Additional chapters discuss case studies on smart cities, brain–computer interfaces (BCI), probabilistic text entry, secure gestures, personal context from mobile phones, building adaptive touch interfaces, and automotive user interfaces (UIs). The chapters on case studies also highlight an in-depth look at domain-specific language (DSL) and ML methods used, for example, in touch, gesture, electroencephalography (EEG), electrocardiography (ECG), and galvanic skin response (GSR) signals, or embedded sensor inputs. A common theme throughout is the ubiquitous support for humans as they go about their daily professional or personal activities.This introductory book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal–multisensor systems. After each chapter an expert on the legal and ethical issues explores the wider ethical issues on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during interaction with novel platforms.Parisa Eslambolchilar, Andreas Komninos, and Mark D. Dunlop, March 2020AcknowledgmentsWe would like to thank our external reviewers for their valuable feedback throughout the writing process.},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {xv–xvi}
}

@inproceedings{10.1145/3123939.3123979,
author = {Park, Jongse and Sharma, Hardik and Mahajan, Divya and Kim, Joon Kyung and Olds, Preston and Esmaeilzadeh, Hadi},
title = {Scale-out Acceleration for Machine Learning},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3123979},
doi = {10.1145/3123939.3123979},
abstract = {The growing scale and complexity of Machine Learning (ML) algorithms has resulted in prevalent use of distributed general-purpose systems. In a rather disjoint effort, the community is focusing mostly on high performance single-node accelerators for learning. This work bridges these two paradigms and offers CoSMIC, a full computing stack constituting language, compiler, system software, template architecture, and circuit generators, that enable programmable acceleration of learning at scale. CoSMIC enables programmers to exploit scale-out acceleration using FPGAs and Programmable ASICs (P-ASICs) from a high-level and mathematical Domain-Specific Language (DSL). Nonetheless, CoSMIC does not require programmers to delve into the onerous task of system software development or hardware design. CoSMIC achieves three conflicting objectives of efficiency, automation, and programmability, by integrating a novel multi-threaded template accelerator architecture and a cohesive stack that generates the hardware and software code from its high-level DSL. CoSMIC can accelerate a wide range of learning algorithms that are most commonly trained using parallel variants of gradient descent. The key is to distribute partial gradient calculations of the learning algorithms across the accelerator-augmented nodes of the scale-out system. Additionally, CoSMIC leverages the parallelizability of the algorithms to offer multi-threaded acceleration within each node. Multi-threading allows CoSMIC to efficiently exploit the numerous resources that are becoming available on modern FPGAs/P-ASICs by striking a balance between multi-threaded parallelism and single-threaded performance. CoSMIC takes advantage of algorithmic properties of ML to offer a specialized system software that optimizes task allocation, role-assignment, thread management, and internode communication. We evaluate the versatility and efficiency of CoSMIC for 10 different machine learning applications from various domains. On average, a 16-node CoSMIC with UltraScale+ FPGAs offers 18.8\texttimes{} speedup over a 16-node Spark system with Xeon processors while the programmer only writes 22--55 lines of code. CoSMIC offers higher scalability compared to the state-of-the-art Spark; scaling from 4 to 16 nodes with CoSMIC yields 2.7\texttimes{} improvements whereas Spark offers 1.8\texttimes{}. These results confirm that the full-stack approach of CoSMIC takes an effective and vital step towards enabling scale-out acceleration for machine learning.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {367–381},
numpages = {15},
keywords = {cloud, machine learning, accelerator, distributed, scale-out},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

