@inproceedings{10.1145/3417990.3420057,
author = {Moin, Armin and R\"{o}ssler, Stephan and Sayih, Marouane and G\"{u}nnemann, Stephan},
title = {From Things' Modeling Language (ThingML) to Things' Machine Learning (ThingML2)},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3420057},
doi = {10.1145/3417990.3420057},
abstract = {In this paper, we illustrate how to enhance an existing state-of-the-art modeling language and tool for the Internet of Things (IoT), called ThingML, to support machine learning on the modeling level. To this aim, we extend the Domain-Specific Language (DSL) of ThingML, as well as its code generation framework. Our DSL allows one to define things, which are in charge of carrying out data analytics. Further, our code generators can automatically produce the complete implementation in Java and Python. The generated Python code is responsible for data analytics and employs APIs of machine learning libraries, such as Keras, Tensorflow and Scikit Learn. Our prototype is available as open source software on Github.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {19},
numpages = {2},
keywords = {internet of things, machine learning, domain-specific modeling},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3503222.3507778,
author = {Jangda, Abhinav and Huang, Jun and Liu, Guodong and Sabet, Amir Hossein Nodehi and Maleki, Saeed and Miao, Youshan and Musuvathi, Madanlal and Mytkowicz, Todd and Saarikivi, Olli},
title = {Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507778},
doi = {10.1145/3503222.3507778},
abstract = {Recent trends towards large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, the current logical separation between computation and communication kernels in machine learning frameworks misses optimization opportunities across this barrier. Breaking this abstraction can provide many optimizations to improve the performance of distributed workloads. However, manually applying these optimizations requires modifying the underlying computation and communication libraries for each scenario, which is both time consuming and error-prone. Therefore, we present CoCoNet, which contains (i) a domain specific language to express a distributed machine learning program in the form of computation and communication operations, (ii) a set of semantics preserving transformations to optimize the program, and (iii) a compiler to generate jointly optimized communication and computation GPU kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNet enabled us to optimize data-, model- and pipeline-parallel workloads in large language models with only a few lines of code. Our experiments show that CoCoNet significantly outperforms state-of-the-art distributed machine learning implementations.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {402–416},
numpages = {15},
keywords = {Code Generation, CUDA, Collective Communication, Compiler Optimizations, Distributed Machine Learning, MPI},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3468044.3468052,
author = {Podobas, Artur and Svedin, Martin and Chien, Steven W. D. and Peng, Ivy B. and Ravichandran, Naresh Balaji and Herman, Pawel and Lansner, Anders and Markidis, Stefano},
title = {StreamBrain: An HPC Framework for Brain-like Neural Networks on CPUs, GPUs and FPGAs},
year = {2021},
isbn = {9781450385497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468044.3468052},
doi = {10.1145/3468044.3468052},
abstract = {The modern deep learning method based on backpropagation has surged in popularity and has been used in multiple domains and application areas. At the same time, there are other - less-known - machine learning algorithms with a mature and solid theoretical foundation whose performance remains unexplored. One such example is the brain-like Bayesian Confidence Propagation Neural Network (BCPNN). In this paper, we introduce StreamBrain--a framework that allows neural networks based on BCPNN to be practically deployed in High-Performance Computing systems. StreamBrain is a domain-specific language (DSL), similar in concept to existing machine learning (ML) frameworks, and supports backends for CPUs, GPUs, and even FPGAs. We empirically demonstrate that StreamBrain can train the well-known ML benchmark dataset MNIST within seconds, and we are the first to demonstrate BCPNN on STL-10 size networks. We also show how StreamBrain can be used to train with custom floating-point formats and illustrate the impact of using different bfloat variations on BCPNN using FPGAs.},
booktitle = {Proceedings of the 11th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
articleno = {8},
numpages = {6},
keywords = {Neural networks, GPU, Emerging Machine Learning, Unsupervised learning, HPC, AI, FPGA, BCPNN, Representation learning},
location = {Online, Germany},
series = {HEART '21}
}

@inproceedings{10.1145/3368826.3377923,
author = {Shaikhha, Amir and Schleich, Maximilian and Ghita, Alexandru and Olteanu, Dan},
title = {Multi-Layer Optimizations for End-to-End Data Analytics},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377923},
doi = {10.1145/3368826.3377923},
abstract = {We consider the problem of training machine learning models over multi-relational data. The mainstream approach is to first construct the training dataset using a feature extraction query over input database and then use a statistical software package of choice to train the model. In this paper we introduce Iterative Functional Aggregate Queries (IFAQ), a framework that realizes an alternative approach. IFAQ treats the feature extraction query and the learning task as one program given in the IFAQ's domain-specific language, which captures a subset of Python commonly used in Jupyter notebooks for rapid prototyping of machine learning applications. The program is subject to several layers of IFAQ optimizations, such as algebraic transformations, loop transformations, schema specialization, data layout optimizations, and finally compilation into efficient low-level C++ code specialized for the given workload and data. We show that a Scala implementation of IFAQ can outperform mlpack, Scikit, and TensorFlow by several orders of magnitude for linear regression and regression tree models over several relational datasets.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {145–157},
numpages = {13},
keywords = {In-Database Machine Learning, Query Compilation, Multi-Query Optimization},
location = {San Diego, CA, USA},
series = {CGO 2020}
}

@inproceedings{10.1145/3470481.3472705,
author = {Chhokra, Ajay and Barreto, Carlos and Dubey, Abhishek and Karsai, Gabor and Koutsoukos, Xenofon},
title = {Power-Attack: A Comprehensive Tool-Chain for Modeling and Simulating Attacks in Power Systems},
year = {2021},
isbn = {9781450386081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470481.3472705},
doi = {10.1145/3470481.3472705},
abstract = {Due to the increased deployment of novel communication, control and protection functions, the grid has become vulnerable to a variety of attacks. Designing robust machine learning based attack detection and mitigation algorithms require large amounts of data that rely heavily on a representative environment, where different attacks can be simulated. This paper presents a comprehensive tool-chain for modeling and simulating attacks in power systems. The paper makes the following contributions, first, we present a probabilistic domain specific language to define multiple attack scenarios and simulation configuration parameters. Secondly, we extend the PyPower-dynamics simulator with protection system components to simulate cyber attacks in control and protection layers of power system. In the end, we demonstrate multiple attack scenarios with a case study based on IEEE 39 bus system.},
booktitle = {Proceedings of the 9th Workshop on Modeling and Simulation of Cyber-Physical Energy Systems},
articleno = {5},
numpages = {6},
keywords = {cyber attacks, protection system, cyber security, power system simulation},
location = {Virtual Event},
series = {MSCPES '21}
}

@inproceedings{10.1145/3018610.3018619,
author = {Jakub\r{u}v, Jan and Urban, Josef},
title = {BliStrTune: Hierarchical Invention of Theorem Proving Strategies},
year = {2017},
isbn = {9781450347051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018610.3018619},
doi = {10.1145/3018610.3018619},
abstract = {Inventing targeted proof search strategies for specific problem sets is a difficult task. State-of-the-art automated theorem provers (ATPs) such as E allow a large number of user-specified proof search strategies described in a rich domain specific language. Several machine learning methods that invent strategies automatically for ATPs were proposed previously. One of them is the Blind Strategymaker (BliStr), a system for automated invention of ATP strategies. In this paper we introduce BliStrTune -- a hierarchical extension of BliStr. BliStrTune allows exploring much larger space of E strategies by interleaving search for high-level parameters with their fine-tuning. We use BliStrTune to invent new strategies based also on new clause weight functions targeted at problems from large ITP libraries. We show that the new strategies significantly improve E's performance in solving problems from the Mizar Mathematical Library.},
booktitle = {Proceedings of the 6th ACM SIGPLAN Conference on Certified Programs and Proofs},
pages = {43–52},
numpages = {10},
keywords = {Clause Weight Functions, Machine Learning, Automated Theorem Proving, Proof Search Heuristics},
location = {Paris, France},
series = {CPP 2017}
}

@inproceedings{10.1145/3461648.3463842,
author = {Poroor, Jayaraj and Lal, Akash and Ghanta, Sandesh},
title = {Robust I/O-Compute Concurrency for Machine Learning Pipelines in Constrained Cyber-Physical Devices},
year = {2021},
isbn = {9781450384728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461648.3463842},
doi = {10.1145/3461648.3463842},
abstract = {Cyberphysical systems have numerous industrial and commercial applications. Such systems are often built using low-resource devices that gather and process data, using machine-learning (ML) models, to make intelligent decisions and provide value to users. Programming such low-resource devices with an impoverished system runtime is often challenging. This paper presents a new domain-specific language called PiCon for programming ML pipelines in low-resource devices. PiCon allows safe I/O-compute concurrency, ruling out a large class of errors, while providing a simple and sequential coding abstraction to the programmer. PiCon compiles to C code and easily interfaces with existing C/C++ code. Furthermore, the generated code does not rely on multi-threading support or dynamic memory allocation, dramatically reducing its footprint on the device. We present experience porting two real-world ML applications that demonstrate simplification in programmability, in addition to several safe-by-construction guarantees.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {1–11},
numpages = {11},
keywords = {Cyberphysical systems, Embedded systems, IoT, Constrained devices, Machine Learning},
location = {Virtual, Canada},
series = {LCTES 2021}
}

@article{10.14778/3007263.3007279,
author = {Boehm, Matthias and Dusenberry, Michael W. and Eriksson, Deron and Evfimievski, Alexandre V. and Manshadi, Faraz Makari and Pansare, Niketan and Reinwald, Berthold and Reiss, Frederick R. and Sen, Prithviraj and Surve, Arvind C. and Tatikonda, Shirish},
title = {SystemML: Declarative Machine Learning on Spark},
year = {2016},
issue_date = {September 2016},
publisher = {VLDB Endowment},
volume = {9},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3007263.3007279},
doi = {10.14778/3007263.3007279},
abstract = {The rising need for custom machine learning (ML) algorithms and the growing data sizes that require the exploitation of distributed, data-parallel frameworks such as MapReduce or Spark, pose significant productivity challenges to data scientists. Apache SystemML addresses these challenges through declarative ML by (1) increasing the productivity of data scientists as they are able to express custom algorithms in a familiar domain-specific language covering linear algebra primitives and statistical functions, and (2) transparently running these ML algorithms on distributed, data-parallel frameworks by applying cost-based compilation techniques to generate efficient, low-level execution plans with in-memory single-node and large-scale distributed operations. This paper describes SystemML on Apache Spark, end to end, including insights into various optimizer and runtime techniques as well as performance characteristics. We also share lessons learned from porting SystemML to Spark and declarative ML in general. Finally, SystemML is open-source, which allows the database community to leverage it as a testbed for further research.},
journal = {Proc. VLDB Endow.},
month = {sep},
pages = {1425–1436},
numpages = {12}
}

@inproceedings{10.1145/3183895.3183901,
author = {Svore, Krysta and Geller, Alan and Troyer, Matthias and Azariah, John and Granade, Christopher and Heim, Bettina and Kliuchnikov, Vadym and Mykhailova, Mariia and Paz, Andres and Roetteler, Martin},
title = {Q#: Enabling Scalable Quantum Computing and Development with a High-Level DSL},
year = {2018},
isbn = {9781450363556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183895.3183901},
doi = {10.1145/3183895.3183901},
abstract = {Quantum computing exploits quantum phenomena such as superposition and entanglement to realize a form of parallelism that is not available to traditional computing. It offers the potential of significant computational speed-ups in quantum chemistry, materials science, cryptography, and machine learning.The dominant approach to programming quantum computers is to provide an existing high-level language with libraries that allow for the expression of quantum programs. This approach can permit computations that are meaningless in a quantum context; prohibits succint expression of interaction between classical and quantum logic; and does not provide important constructs that are required for quantum programming.We present Q#, a quantum-focused domain-specific language explicitly designed to correctly, clearly and completely express quantum algorithms. Q# provides a type system; a tightly constrained environment to safely interleave classical and quantum computations; specialized syntax; symbolic code manipulation to automatically generate correct transformations of quantum operations; and powerful functional constructs which aid composition.},
booktitle = {Proceedings of the Real World Domain Specific Languages Workshop 2018},
articleno = {7},
numpages = {10},
keywords = {domain specific language, quantum computing, functional programming},
location = {Vienna, Austria},
series = {RWDSL2018}
}

@article{10.14778/3430915.3430919,
author = {Kiefer, Martin and Poulakis, Ilias and Bre\ss{}, Sebastian and Markl, Volker},
title = {Scotch: Generating FPGA-Accelerators for Sketching at Line Rate},
year = {2021},
issue_date = {November 2020},
publisher = {VLDB Endowment},
volume = {14},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3430915.3430919},
doi = {10.14778/3430915.3430919},
abstract = {Sketching algorithms are a powerful tool for single-pass data summarization. Their numerous applications include approximate query processing, machine learning, and large-scale network monitoring. In the presence of high-bandwidth interconnects or in-memory data, the throughput of summary maintenance over input data becomes the bottleneck. While FPGAs have shown admirable throughput and energy-efficiency for data processing tasks, developing FPGA accelerators requires a sophisticated hardware design and expensive manual tuning by an expert.We propose Scotch, a novel system for accelerating sketch maintenance using FPGAs. Scotch provides a domain-specific language for the user-friendly, high-level definition of a broad class of sketching algorithms. A code generator performs the heavy-lifting of hardware description, while an auto-tuning algorithm optimizes the summary size. Our evaluation shows that FPGA accelerators generated by Scotch outperform CPU- and GPU-based sketching by up to two orders of magnitude in terms of throughput and up to a factor of five in terms of energy efficiency.},
journal = {Proc. VLDB Endow.},
month = {dec},
pages = {281–293},
numpages = {13}
}

@inproceedings{10.1145/3097983.3098171,
author = {Cheng, Heng-Tze and Haque, Zakaria and Hong, Lichan and Ispir, Mustafa and Mewald, Clemens and Polosukhin, Illia and Roumpos, Georgios and Sculley, D. and Smith, Jamie and Soergel, David and Tang, Yuan and Tucker, Philipp and Wicke, Martin and Xia, Cassandra and Xie, Jianwei},
title = {TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098171},
doi = {10.1145/3097983.3098171},
abstract = {We present a framework for specifying, training, evaluating, and deploying machine learning models. Our focus is on simplifying cutting edge machine learning for practitioners in order to bring such technologies into production. Recognizing the fast evolution of the field of deep learning, we make no attempt to capture the design space of all possible model architectures in a domain-specific language (DSL) or similar configuration language. We allow users to write code to define their models, but provide abstractions that guide developers to write models in ways conducive to productionization. We also provide a unifying Estimator interface, making it possible to write downstream infrastructure (e.g. distributed training, hyperparameter tuning) independent of the model implementation.We balance the competing demands for flexibility and simplicity by offering APIs at different levels of abstraction, making common model architectures available out of the box, while providing a library of utilities designed to speed up experimentation with model architectures. To make out of the box models flexible and usable across a wide range of problems, these canned Estimators are parameterized not only over traditional hyperparameters, but also using feature columns, a declarative specification describing how to interpret input dataWe discuss our experience in using this framework in research and production environments, and show the impact on code health, maintainability, and development speed.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1763–1771},
numpages = {9},
keywords = {deep learning, high level api, machine learning framework},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3192366.3192379,
author = {Koeplinger, David and Feldman, Matthew and Prabhakar, Raghu and Zhang, Yaqi and Hadjis, Stefan and Fiszel, Ruben and Zhao, Tian and Nardi, Luigi and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Spatial: A Language and Compiler for Application Accelerators},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192379},
doi = {10.1145/3192366.3192379},
abstract = {Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs for improved performance and energy efficiency. Unfortunately, adoption of these architectures has been limited by their programming models. HDLs lack abstractions for productivity and are difficult to target from higher level languages. HLS tools are more productive, but offer an ad-hoc mix of software and hardware abstractions which make performance optimizations difficult. In this work, we describe a new domain-specific language and compiler called Spatial for higher level descriptions of application accelerators. We describe Spatial's hardware-centric abstractions for both programmer productivity and design performance, and summarize the compiler passes required to support these abstractions, including pipeline scheduling, automatic memory banking, and automated design tuning driven by active machine learning. We demonstrate the language's ability to target FPGAs and CGRAs from common source code. We show that applications written in Spatial are, on average, 42% shorter and achieve a mean speedup of 2.9x over SDAccel HLS when targeting a Xilinx UltraScale+ VU9P FPGA on an Amazon EC2 F1 instance.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {296–311},
numpages = {16},
keywords = {FPGAs, domain-specific languages, CGRAs, high-level synthesis, compilers, hardware accelerators, reconfigurable architectures},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192379,
author = {Koeplinger, David and Feldman, Matthew and Prabhakar, Raghu and Zhang, Yaqi and Hadjis, Stefan and Fiszel, Ruben and Zhao, Tian and Nardi, Luigi and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Spatial: A Language and Compiler for Application Accelerators},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192379},
doi = {10.1145/3296979.3192379},
abstract = {Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs for improved performance and energy efficiency. Unfortunately, adoption of these architectures has been limited by their programming models. HDLs lack abstractions for productivity and are difficult to target from higher level languages. HLS tools are more productive, but offer an ad-hoc mix of software and hardware abstractions which make performance optimizations difficult. In this work, we describe a new domain-specific language and compiler called Spatial for higher level descriptions of application accelerators. We describe Spatial's hardware-centric abstractions for both programmer productivity and design performance, and summarize the compiler passes required to support these abstractions, including pipeline scheduling, automatic memory banking, and automated design tuning driven by active machine learning. We demonstrate the language's ability to target FPGAs and CGRAs from common source code. We show that applications written in Spatial are, on average, 42% shorter and achieve a mean speedup of 2.9x over SDAccel HLS when targeting a Xilinx UltraScale+ VU9P FPGA on an Amazon EC2 F1 instance.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {296–311},
numpages = {16},
keywords = {CGRAs, hardware accelerators, reconfigurable architectures, compilers, domain-specific languages, high-level synthesis, FPGAs}
}

@inproceedings{10.1145/3331543.3342587,
author = {Melkonian, Orestis and Ren, Iris Yuping and Swierstra, Wouter and Volk, Anja},
title = {What Constitutes a Musical Pattern?},
year = {2019},
isbn = {9781450368117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331543.3342587},
doi = {10.1145/3331543.3342587},
abstract = {There is a plethora of computational systems designed for alagorithmic discovery of musical patterns, ranging from geometrical methods to machine learning based approaches. These algorithms often disagree on what constitutes a pattern, mainly due to the lack of a broadly accepted definition of musical patterns. On the other side of the spectrum, human-annotated musical patterns also often do not reach a consensus, partly due to the subjectivity of each individual expert, but also due to the elusive definition of a musical pattern in general. In this work, we propose a framework of music-theoretic transformations, through which one can easily define predicates which dictate when two musical patterns belong to a particular equivalence class. We exploit simple notions from category theory to assemble transformations compositionally, allowing us to define complex transformations from simple and well-understood ones. Additionally, we provide a prototype implementation of our theoretical framework as an embedded domain-specific language in Haskell and conduct a meta-analysis on several algorithms submitted to a pattern extraction task of the the Music Information Retrieval Evaluation eXchange (MIREX) over the previous years.},
booktitle = {Proceedings of the 7th ACM SIGPLAN International Workshop on Functional Art, Music, Modeling, and Design},
pages = {95–105},
numpages = {11},
keywords = {transformation, edit distance, evaluation, clustering, musical patterns, contravariance},
location = {Berlin, Germany},
series = {FARM 2019}
}

@article{10.14778/3342263.3342633,
author = {Kunft, Andreas and Katsifodimos, Asterios and Schelter, Sebastian and Bre\ss{}, Sebastian and Rabl, Tilmann and Markl, Volker},
title = {An Intermediate Representation for Optimizing Machine Learning Pipelines},
year = {2019},
issue_date = {July 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3342263.3342633},
doi = {10.14778/3342263.3342633},
abstract = {Machine learning (ML) pipelines for model training and validation typically include preprocessing, such as data cleaning and feature engineering, prior to training an ML model. Preprocessing combines relational algebra and user-defined functions (UDFs), while model training uses iterations and linear algebra. Current systems are tailored to either of the two. As a consequence, preprocessing and ML steps are optimized in isolation. To enable holistic optimization of ML training pipelines, we present Lara, a declarative domain-specific language for collections and matrices. Lara's inter-mediate representation (IR) reflects on the complete program, i.e., UDFs, control flow, and both data types. Two views on the IR enable diverse optimizations. Monads enable operator pushdown and fusion across type and loop boundaries. Combinators provide the semantics of domain-specific operators and optimize data access and cross-validation of ML algorithms. Our experiments on preprocessing pipelines and selected ML algorithms show the effects of our proposed optimizations on dense and sparse data, which achieve speedups of up to an order of magnitude.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1553–1567},
numpages = {15}
}

@inproceedings{10.1145/3314221.3314597,
author = {Gopinath, Sridhar and Ghanathe, Nikhil and Seshadri, Vivek and Sharma, Rahul},
title = {Compiling KB-Sized Machine Learning Models to Tiny IoT Devices},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314597},
doi = {10.1145/3314221.3314597},
abstract = {Recent advances in machine learning (ML) have produced KiloByte-size models that can directly run on constrained IoT devices. This approach avoids expensive communication between IoT devices and the cloud, thereby enabling energy-efficient real-time analytics. However, ML models are expressed typically in floating-point, and IoT hardware typically does not support floating-point. Therefore, running these models on IoT devices requires simulating IEEE-754 floating-point using software, which is very inefficient. We present SeeDot, a domain-specific language to express ML inference algorithms and a compiler that compiles SeeDot programs to fixed-point code that can efficiently run on constrained IoT devices. We propose 1)&nbsp;a novel compilation strategy that reduces the search space for some key parameters used in the fixed-point code, and 2)&nbsp;new efficient implementations of expensive operations. SeeDot compiles state-of-the-art KB-sized models to various microcontrollers and low-end FPGAs. We show that SeeDot outperforms 1) software emulation of floating-point (Arduino), 2) high-bitwidth fixed-point (MATLAB), 3) post-training quantization (TensorFlow-Lite), and 4) floating- and fixed-point FPGA implementations generated using high-level synthesis tools.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {79–95},
numpages = {17},
keywords = {IoT device, Programming Language, FPGA, Compiler, Machine Learning, Microcontroller, Fixed-point},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3131851.3131853,
author = {Gulwani, Sumit},
title = {Programming by Examples: Applications, Algorithms, and Ambiguity Resolution},
year = {2017},
isbn = {9781450352918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131851.3131853},
doi = {10.1145/3131851.3131853},
abstract = {99% of computer users do not know programming and hence struggle with repetitive tasks. Programming by Examples (PBE) can revolutionize this landscape by enabling users to synthesize intended programs from example based specifications. A key technical challenge in PBE is to search for programs that are consistent with the examples provided by the user. Our efficient search methodology is based on two key ideas: (i) Restriction of the search space to an appropriate domain-specific language (ii) A divide-and-conquer based search paradigm that inductively reduces the problem of synthesizing a program with a certain top-level operator to simpler synthesis problems over its sub-programs by leveraging the operator's inverse semantics. Another challenge in PBE is to resolve the ambiguity in the example based specification. Our ambiguity resolution methodology leverages two complementary approaches: (a) machine learning based ranking techniques that can pick an intended program from among those that satisfy the specification, and (b) active-learning based user interaction models. I will illustrate these various concepts using Flash Fill, FlashExtract, and FlashRelate---PBE technologies for data manipulation domains. These technologies, which have been released inside various Microsoft products, are useful for data scientists who spend 80% of their time wrangling with data. The Microsoft PROSE SDK allows easy construction of such technologies.},
booktitle = {Proceedings of the 19th International Symposium on Principles and Practice of Declarative Programming},
pages = {2},
numpages = {1},
location = {Namur, Belgium},
series = {PPDP '17}
}

@inproceedings{10.1145/3123939.3123979,
author = {Park, Jongse and Sharma, Hardik and Mahajan, Divya and Kim, Joon Kyung and Olds, Preston and Esmaeilzadeh, Hadi},
title = {Scale-out Acceleration for Machine Learning},
year = {2017},
isbn = {9781450349529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123939.3123979},
doi = {10.1145/3123939.3123979},
abstract = {The growing scale and complexity of Machine Learning (ML) algorithms has resulted in prevalent use of distributed general-purpose systems. In a rather disjoint effort, the community is focusing mostly on high performance single-node accelerators for learning. This work bridges these two paradigms and offers CoSMIC, a full computing stack constituting language, compiler, system software, template architecture, and circuit generators, that enable programmable acceleration of learning at scale. CoSMIC enables programmers to exploit scale-out acceleration using FPGAs and Programmable ASICs (P-ASICs) from a high-level and mathematical Domain-Specific Language (DSL). Nonetheless, CoSMIC does not require programmers to delve into the onerous task of system software development or hardware design. CoSMIC achieves three conflicting objectives of efficiency, automation, and programmability, by integrating a novel multi-threaded template accelerator architecture and a cohesive stack that generates the hardware and software code from its high-level DSL. CoSMIC can accelerate a wide range of learning algorithms that are most commonly trained using parallel variants of gradient descent. The key is to distribute partial gradient calculations of the learning algorithms across the accelerator-augmented nodes of the scale-out system. Additionally, CoSMIC leverages the parallelizability of the algorithms to offer multi-threaded acceleration within each node. Multi-threading allows CoSMIC to efficiently exploit the numerous resources that are becoming available on modern FPGAs/P-ASICs by striking a balance between multi-threaded parallelism and single-threaded performance. CoSMIC takes advantage of algorithmic properties of ML to offer a specialized system software that optimizes task allocation, role-assignment, thread management, and internode communication. We evaluate the versatility and efficiency of CoSMIC for 10 different machine learning applications from various domains. On average, a 16-node CoSMIC with UltraScale+ FPGAs offers 18.8\texttimes{} speedup over a 16-node Spark system with Xeon processors while the programmer only writes 22--55 lines of code. CoSMIC offers higher scalability compared to the state-of-the-art Spark; scaling from 4 to 16 nodes with CoSMIC yields 2.7\texttimes{} improvements whereas Spark offers 1.8\texttimes{}. These results confirm that the full-stack approach of CoSMIC takes an effective and vital step towards enabling scale-out acceleration for machine learning.},
booktitle = {Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {367–381},
numpages = {15},
keywords = {distributed, cloud, scale-out, accelerator, machine learning},
location = {Cambridge, Massachusetts},
series = {MICRO-50 '17}
}

@inproceedings{10.1145/3314221.3314633,
author = {Fremont, Daniel J. and Dreossi, Tommaso and Ghosh, Shromona and Yue, Xiangyu and Sangiovanni-Vincentelli, Alberto L. and Seshia, Sanjit A.},
title = {Scenic: A Language for Scenario Specification and Scene Generation},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314633},
doi = {10.1145/3314221.3314633},
abstract = {We propose a new probabilistic programming language for the design and analysis of perception systems, especially those based on machine learning. Specifically, we consider the problems of training a perception system to handle rare events, testing its performance under different conditions, and debugging failures. We show how a probabilistic programming language can help address these problems by specifying distributions encoding interesting types of inputs and sampling these to generate specialized training and test sets. More generally, such languages can be used for cyber-physical systems and robotics to write environment models, an essential prerequisite to any formal analysis. In this paper, we focus on systems like autonomous cars and robots, whose environment is a scene, a configuration of physical objects and agents. We design a domain-specific language, Scenic, for describing scenarios that are distributions over scenes. As a probabilistic programming language, Scenic allows assigning distributions to features of the scene, as well as declaratively imposing hard and soft constraints over the scene. We develop specialized techniques for sampling from the resulting distribution, taking advantage of the structure provided by Scenic's domain-specific syntax. Finally, we apply Scenic in a case study on a convolutional neural network designed to detect cars in road images, improving its performance beyond that achieved by state-of-the-art synthetic data generation methods.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {63–78},
numpages = {16},
keywords = {deep learning, automatic test generation, probabilistic programming, fuzz testing, synthetic data, scenario description language},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inbook{10.1145/3447404.3447405,
title = {Preface},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447405},
abstract = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice provides a comprehensive resource on what has become the dominant paradigm for novel interaction design methods involving gesture, speech, text, and touch embedded in novel and emerging interfaces. These interfaces support smartphones, wearables, in-vehicle devices, virtual reality, robotic, the Internet of Things (IoT), brain–computer interaction, and many other applications that are now highly competitive commercially.This edited collection is written by international experts and pioneers in the field of digital signal processing (DSP) and machine learning (ML) for interactive systems. It provides a textbook for students, and a reference and technology roadmap for developers and professionals working in interaction design on emerging platforms. This introductory textbook presents theory chapters on statistical grounding, signal processing, and ML foundations for guiding the development of novel interactive systems. Additional chapters discuss case studies on smart cities, brain–computer interfaces (BCI), probabilistic text entry, secure gestures, personal context from mobile phones, building adaptive touch interfaces, and automotive user interfaces (UIs). The chapters on case studies also highlight an in-depth look at domain-specific language (DSL) and ML methods used, for example, in touch, gesture, electroencephalography (EEG), electrocardiography (ECG), and galvanic skin response (GSR) signals, or embedded sensor inputs. A common theme throughout is the ubiquitous support for humans as they go about their daily professional or personal activities.This introductory book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal–multisensor systems. After each chapter an expert on the legal and ethical issues explores the wider ethical issues on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during interaction with novel platforms.Parisa Eslambolchilar, Andreas Komninos, and Mark D. Dunlop, March 2020AcknowledgmentsWe would like to thank our external reviewers for their valuable feedback throughout the writing process.},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {xv–xvi}
}

@article{10.14778/3236187.3236188,
author = {Mahajan, Divya and Kim, Joon Kyung and Sacks, Jacob and Ardalan, Adel and Kumar, Arun and Esmaeilzadeh, Hadi},
title = {In-RDBMS Hardware Acceleration of Advanced Analytics},
year = {2018},
issue_date = {July 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3236187.3236188},
doi = {10.14778/3236187.3236188},
abstract = {The data revolution is fueled by advances in machine learning, databases, and hardware design. Programmable accelerators are making their way into each of these areas independently. As such, there is a void of solutions that enables hardware acceleration at the intersection of these disjoint fields. This paper sets out to be the initial step towards a unifying solution for in-Database Acceleration of Advanced Analytics (DAnA). Deploying specialized hardware, such as FPGAs, for in-database analytics currently requires hand-designing the hardware and manually routing the data. Instead, DAnA automatically maps a high-level specification of advanced analytics queries to an FPGA accelerator. The accelerator implementation is generated for a User Defined Function (UDF), expressed as a part of an SQL query using a Python-embedded Domain-Specific Language (DSL). To realize an efficient in-database integration, DAnA accelerators contain a novel hardware structure, Striders, that directly interface with the buffer pool of the database. Striders extract, cleanse, and process the training data tuples that are consumed by a multi-threaded FPGA engine that executes the analytics algorithm. We integrate DAnA with PostgreSQL to generate hardware accelerators for a range of real-world and synthetic datasets running diverse ML algorithms. Results show that DAnA-enhanced PostgreSQL provides, on average, 8.3\texttimes{} end-to-end speedup for real datasets, with a maximum of 28.2\texttimes{}. Moreover, DAnA-enhanced PostgreSQL is, on average, 4.0\texttimes{} faster than the multi-threaded Apache MADLib running on Greenplum. DAnA provides these benefits while hiding the complexity of hardware design from data scientists and allowing them to express the algorithm in ≈30-60 lines of Python.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {1317–1331},
numpages = {15}
}

